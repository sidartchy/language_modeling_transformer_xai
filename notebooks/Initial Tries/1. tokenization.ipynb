{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca15d1b",
   "metadata": {},
   "source": [
    "# Initial Try for tokenizing using Google's SentencePiece\n",
    "\n",
    "Will only use one txt file for now :) If everything goes well, will test on larger data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b799da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c6b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing only\n",
    "input_path = \"../../data/raw/academic/file1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e227d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input=input_path,\n",
    "    model_prefix='tokenizer',\n",
    "    vocab_size=8000,               # Or 16k/32k if your dataset is larger\n",
    "    model_type='bpe',              # Can also try: 'unigram', 'word', 'char'\n",
    "    character_coverage=1.0,        # For English, 1.0 is fine\n",
    "    pad_id=0, unk_id=1, bos_id=2, eos_id=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd57b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc7ba1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184, 7854, 349, 73, 5, 2086, 27, 4430, 7851, 1786]\n"
     ]
    }
   ],
   "source": [
    "# trying to encode a sentence\n",
    "encoded = sp.encode(\"Hi This is a test of encoding ...\", out_type=int)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "112d37b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi This is a test of encoding ...'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now decoding the encoded sentence \n",
    "decoded = sp.decode(encoded)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3fb01",
   "metadata": {},
   "source": [
    "Also may be store the token ids in a bin in disk : for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3efe4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061cce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ff62526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54f7be82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 21cm signal from the Epoch of Reionization (EoR) is observed as a three-dimensional data set known as a lightcone, c'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad5666ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 3077, 4615, 2115, 209, 12, 6642, 27, 7661, 58]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text, store the ids for later use\n",
    "token_ids = sp.encode(text, out_type=int)\n",
    "# print first 10 ids\n",
    "token_ids[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed8c230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id_bin_path = \"../../data/processed/Initial/initial_token_ids.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd13df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(token_ids, dtype=np.uint16).tofile(token_id_bin_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431ffa2",
   "metadata": {},
   "source": [
    "**The bin has been saved successfully inside ```data/processed/Initial```. Use that later while creating CustomDataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2417cf7",
   "metadata": {},
   "source": [
    "Seems working. Now We'll store these ids "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade98a4",
   "metadata": {},
   "source": [
    "Noice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588437a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
