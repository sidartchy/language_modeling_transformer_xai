{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10302971",
   "metadata": {},
   "source": [
    "### Single Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762eec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f00462e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2106a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for the transformer model\n",
    "\n",
    "n_embed = 120\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "head_size = n_embed // n_heads\n",
    "batch_size = 16  # Batch size for training\n",
    "block_size = 256  # Context size for the model\n",
    "dropout = 0.2  # Dropout rate for regularization\n",
    "vocab_size = 8000\n",
    "\n",
    "learning_rate = 3e-4 \n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e23a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Data ( Copied from 2. Data Preparation )\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "token_id_bin_path = \"../../data/processed/Initial/initial_token_ids.bin\"\n",
    "token_ids = np.fromfile(token_id_bin_path, dtype=np.uint16)\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, token_ids, block_size ):\n",
    "        self.block_size = block_size   \n",
    "        self.data = np.array(token_ids, dtype=np.uint16)  # our data is going to be an np array ( for easy slicing )\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data).shape[0] - self.block_size   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.tensor(self.data[idx:idx+self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1 : idx+self.block_size + 1], dtype = torch.long)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "## Lets just quickly split the dataset  -- first 80% be train data\n",
    "split_idx = int(0.8 * len(token_ids))\n",
    "train_token_ids = token_ids[:split_idx]\n",
    "val_token_ids = token_ids[split_idx:]\n",
    "\n",
    "len(train_token_ids), len(val_token_ids)\n",
    "token_dataset = TokenDataset(train_token_ids, block_size)\n",
    "trainloader = DataLoader(token_dataset, batch_size = 32, shuffle=True, drop_last = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbc8b5",
   "metadata": {},
   "source": [
    "We know that for each token it consist a embedding vector of dimension n_embed.\n",
    "\n",
    "i.e.. Ei vector of size n_embed*1                ,where i runs to block_size\n",
    "\n",
    "And for each head there is a query matrix and key matrix of size head_size*n_embed. \n",
    "Which is applied to same x for self-head attention\n",
    "\n",
    "and Qi = Wq * Ei = head_size*1 for each block_size and batch_size\n",
    "\n",
    "It can be represented as Linear(n_embed,head_size)\n",
    "\n",
    "How much each query vector attends to key vector is represented from dot product of Ki.Qi at each cell of matrix of size TxT\n",
    "\n",
    "this is represented by \n",
    "Attend = query @ key\n",
    "\n",
    "and the x is represented with the down projection to the dimension of head_size which is concatenated later\n",
    "Vi = Wv * Ei\n",
    "\n",
    "output from single head = attend @ Vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embed = n_embed\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embed, head_size)\n",
    "        self.query = nn.Linear(n_embed, head_size)\n",
    "        self.value = nn.Linear(n_embed, head_size)\n",
    "        self.register_buffer('trill', torch.tril(torch.ones(block_size, block_size)))  # Lower triangular matrix for masking\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape  # B is Batch_size, T is Block_size, C is n_embed\n",
    "        # x is a shape of Batch_size x Block_size x n_embed\n",
    "        key= self.key(x)        # B,T,H = head_size\n",
    "        query = self.query(x)   # B,T,H = head_size\n",
    "\n",
    "        # B,T,H @ B,H,T\n",
    "        attend = query @ key.transpose(-2, -1)  # B,T,T\n",
    "\n",
    "        attend = attend / (self.head_size ** 0.5)  #  Scaled Dot-Product Attention Attention(Q,K,V)=softmax(QK^T/sqrt(d_k))V\n",
    "\n",
    "        # trill = torch.tril(torch.ones(attend.shape[-1], attend.shape[-1]))  # Lower triangular matrix of block_size\n",
    "\n",
    "        attend = attend.masked_fill(self.trill[:T, :T] == 0, float('-inf'))  # Masking future tokens\n",
    "\n",
    "        attend = torch.softmax(attend, dim=-1) # Column-wise softmax IG\n",
    "\n",
    "        value = self.value(x) # B,T,H  \n",
    "\n",
    "        out = attend @ value  # B,T,H\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b9ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = n_embed // n_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(n_embed, self.head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)  # Concatenate outputs from all heads\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f2de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feed_forward(nn.Module):\n",
    "\n",
    "    # Multi-layer perceptron (MLP) for feed-forward network in transformer\n",
    "    \n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "        nn.Linear(n_embed, 4 * n_embed),  # Up-projection min of 4* n_embed from the paper Attention Is All You Need\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * n_embed, n_embed),  # Down-projection back to n_embed\n",
    "        nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.network(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "606d06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Bloack of the Transformer\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_embed, n_heads)\n",
    "        self.feed_forward = feed_forward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Creating a residual connection around the attention layer\n",
    "\n",
    "        x = x + self.attention(self.ln1(x))  # Layer normalization before attention\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471194a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10e7a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embed)  # Token embeddings\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embed)\n",
    " \n",
    "        self.block = nn.ModuleList([Block(n_embed, n_heads) for _ in range(n_layers)])\n",
    "        self.block = nn.Sequential(*self.block) # Sequentially stacking the blocks\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(n_embed)  # Final layer normalization\n",
    "\n",
    "        self.linear = nn.Linear(n_embed, vocab_size)  # Output layer for vocabulary size\n",
    "\n",
    "    def forward(self, x,target=None):\n",
    "\n",
    "        x = x.long() ## Ensuring the x is of type long for embedding lookup    \n",
    "        positions = torch.arange(0, x.size(1), device=x.device, dtype=torch.long)\n",
    "\n",
    "    \n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "\n",
    "\n",
    "        x = self.block(x)  # Passing through the transformer blocks\n",
    "\n",
    "        x = self.layer_norm(x)  # Final layer normalization\n",
    "        logits =self.linear(x)  # Output layer to get logits for vocabulary size. B,T,V\n",
    "        output = torch.softmax(logits,dim=-1)  # Applying softmax to get probabilities for each token in the vocabulary \n",
    "         \n",
    "        if target is not None:\n",
    " \n",
    "            logits = logits.view(-1, logits.size(-1))  # Reshape logits to (B*T, V)\n",
    "            target = target.view(-1)  # Reshape target to (B*T)\n",
    "            loss = nn.CrossEntropyLoss()(logits, target)\n",
    "            return output, loss  \n",
    "        else: \n",
    "            loss = None\n",
    "            return output, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "\n",
    "        #idx = Batch_size x Block_size ... B,T\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # idx is of shape (Batch_size, Block_size)  \n",
    "            idx = idx[:,-block_size] #Keep only last block_size tokens\n",
    "\n",
    "            logits = self(idx)  # Get logits for the current input\n",
    "\n",
    "            logits = logits[:,-1,:] # Get logit for the last token in the sequence of each Batch with shape `(Batch_size, Vocab_size)`\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)  # Apply softmax to get probabilities in the vocabulary dimension\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample from the distribution to get the next token \n",
    "\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # Append the next token to the sequence\n",
    "\n",
    "        return idx  # Return the generated sequence of tokens\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cbdf9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding): Embedding(8000, 120)\n",
       "  (position_embedding): Embedding(256, 120)\n",
       "  (block): Sequential(\n",
       "    (0): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SingleHeadAttention(\n",
       "            (key): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (query): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (value): Linear(in_features=120, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=120, out_features=120, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward): feed_forward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=480, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=480, out_features=120, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SingleHeadAttention(\n",
       "            (key): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (query): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (value): Linear(in_features=120, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=120, out_features=120, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward): feed_forward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=480, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=480, out_features=120, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SingleHeadAttention(\n",
       "            (key): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (query): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (value): Linear(in_features=120, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=120, out_features=120, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward): feed_forward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=480, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=480, out_features=120, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SingleHeadAttention(\n",
       "            (key): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (query): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (value): Linear(in_features=120, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=120, out_features=120, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward): feed_forward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=480, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=480, out_features=120, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SingleHeadAttention(\n",
       "            (key): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (query): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (value): Linear(in_features=120, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=120, out_features=120, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward): feed_forward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=480, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=480, out_features=120, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SingleHeadAttention(\n",
       "            (key): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (query): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (value): Linear(in_features=120, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=120, out_features=120, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward): feed_forward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=480, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=480, out_features=120, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SingleHeadAttention(\n",
       "            (key): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (query): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (value): Linear(in_features=120, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=120, out_features=120, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward): feed_forward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=480, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=480, out_features=120, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Block(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-7): 8 x SingleHeadAttention(\n",
       "            (key): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (query): Linear(in_features=120, out_features=15, bias=True)\n",
       "            (value): Linear(in_features=120, out_features=15, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=120, out_features=120, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed_forward): feed_forward(\n",
       "        (network): Sequential(\n",
       "          (0): Linear(in_features=120, out_features=480, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=480, out_features=120, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((120,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=120, out_features=8000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74714815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "Transformer                                             --\n",
       "├─Embedding: 1-1                                        960,000\n",
       "├─Embedding: 1-2                                        30,720\n",
       "├─Sequential: 1-3                                       --\n",
       "│    └─Block: 2-1                                       --\n",
       "│    │    └─MultiHeadAttention: 3-1                     58,080\n",
       "│    │    └─feed_forward: 3-2                           115,800\n",
       "│    │    └─LayerNorm: 3-3                              240\n",
       "│    │    └─LayerNorm: 3-4                              240\n",
       "│    └─Block: 2-2                                       --\n",
       "│    │    └─MultiHeadAttention: 3-5                     58,080\n",
       "│    │    └─feed_forward: 3-6                           115,800\n",
       "│    │    └─LayerNorm: 3-7                              240\n",
       "│    │    └─LayerNorm: 3-8                              240\n",
       "│    └─Block: 2-3                                       --\n",
       "│    │    └─MultiHeadAttention: 3-9                     58,080\n",
       "│    │    └─feed_forward: 3-10                          115,800\n",
       "│    │    └─LayerNorm: 3-11                             240\n",
       "│    │    └─LayerNorm: 3-12                             240\n",
       "│    └─Block: 2-4                                       --\n",
       "│    │    └─MultiHeadAttention: 3-13                    58,080\n",
       "│    │    └─feed_forward: 3-14                          115,800\n",
       "│    │    └─LayerNorm: 3-15                             240\n",
       "│    │    └─LayerNorm: 3-16                             240\n",
       "│    └─Block: 2-5                                       --\n",
       "│    │    └─MultiHeadAttention: 3-17                    58,080\n",
       "│    │    └─feed_forward: 3-18                          115,800\n",
       "│    │    └─LayerNorm: 3-19                             240\n",
       "│    │    └─LayerNorm: 3-20                             240\n",
       "│    └─Block: 2-6                                       --\n",
       "│    │    └─MultiHeadAttention: 3-21                    58,080\n",
       "│    │    └─feed_forward: 3-22                          115,800\n",
       "│    │    └─LayerNorm: 3-23                             240\n",
       "│    │    └─LayerNorm: 3-24                             240\n",
       "│    └─Block: 2-7                                       --\n",
       "│    │    └─MultiHeadAttention: 3-25                    58,080\n",
       "│    │    └─feed_forward: 3-26                          115,800\n",
       "│    │    └─LayerNorm: 3-27                             240\n",
       "│    │    └─LayerNorm: 3-28                             240\n",
       "│    └─Block: 2-8                                       --\n",
       "│    │    └─MultiHeadAttention: 3-29                    58,080\n",
       "│    │    └─feed_forward: 3-30                          115,800\n",
       "│    │    └─LayerNorm: 3-31                             240\n",
       "│    │    └─LayerNorm: 3-32                             240\n",
       "├─LayerNorm: 1-4                                        240\n",
       "├─Linear: 1-5                                           968,000\n",
       "================================================================================\n",
       "Total params: 3,353,840\n",
       "Trainable params: 3,353,840\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abb6f7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaf9f6de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [SingleHeadAttention] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[32m     20\u001b[39m     X, y = X.to(device, dtype=torch.long), y.to(device, dtype=torch.long)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Forward pass \u001b[39;00m\n\u001b[32m     22\u001b[39m     optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m     loss.backward() \n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, target)\u001b[39m\n\u001b[32m     19\u001b[39m positions = torch.arange(\u001b[32m0\u001b[39m, x.size(\u001b[32m1\u001b[39m), device=x.device, dtype=torch.long)\n\u001b[32m     22\u001b[39m x = \u001b[38;5;28mself\u001b[39m.token_embedding(x) + \u001b[38;5;28mself\u001b[39m.position_embedding(positions)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Passing through the transformer blocks\u001b[39;00m\n\u001b[32m     27\u001b[39m x = \u001b[38;5;28mself\u001b[39m.layer_norm(x)  \u001b[38;5;66;03m# Final layer normalization\u001b[39;00m\n\u001b[32m     28\u001b[39m logits =\u001b[38;5;28mself\u001b[39m.linear(x)  \u001b[38;5;66;03m# Output layer to get logits for vocabulary size. B,T,V\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Creating a residual connection around the attention layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Layer normalization before attention\u001b[39;00m\n\u001b[32m     17\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.feed_forward(\u001b[38;5;28mself\u001b[39m.ln2(x))\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out = torch.cat([\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Concatenate outputs from all heads\u001b[39;00m\n\u001b[32m     15\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.proj(out)\n\u001b[32m     16\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\personalProjects\\language_modeling_transformer_xai\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:387\u001b[39m, in \u001b[36m_forward_unimplemented\u001b[39m\u001b[34m(self, *input)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, *\u001b[38;5;28minput\u001b[39m: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    377\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[32m    378\u001b[39m \n\u001b[32m    379\u001b[39m \u001b[33;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    385\u001b[39m \u001b[33;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[32m    386\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    388\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] is missing the required \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m function\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    389\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Module [SingleHeadAttention] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    # if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # # sample a batch of data\n",
    "    # xb, yb = get_batch('train')\n",
    "\n",
    "    # # evaluate the loss\n",
    "    # logits, loss = model(xb, yb)\n",
    "    # optimizer.zero_grad(set_to_none=True)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (X, y) in enumerate(trainloader):\n",
    "        X, y = X.to(device, dtype=torch.long), y.to(device, dtype=torch.long)\n",
    "        logits, loss = model(X, y) # Forward pass \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        if batch_idx % 1 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9ada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion function for custom transformer \n",
    "def criterion(output, target):\n",
    "    \"\"\"\"\n",
    "    Custom criterion function for the transformer model.\n",
    "    Args: \n",
    "        output (torch.Tensor): the output tensor from the model with shape (B, T, vocab_size)\n",
    "        target (torch.Tensor): the target tensor with shape (B, T)\n",
    "    Returns: \n",
    "        loss\n",
    "    \n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
