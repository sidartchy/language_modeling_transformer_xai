{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10302971",
   "metadata": {},
   "source": [
    "### Single Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "762eec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for the transformer model\n",
    "n_embed = 120\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "head_size = n_embed // n_heads\n",
    "block_size = 128  # Context size for the model\n",
    "dropout = 0.2  # Dropout rate for regularization\n",
    "vocab_size = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbc8b5",
   "metadata": {},
   "source": [
    "We know that for each token it consist a embedding vector of dimension n_embed.\n",
    "\n",
    "i.e.. Ei vector of size n_embed*1                ,where i runs to block_size\n",
    "\n",
    "And for each head there is a query matrix and key matrix of size head_size*n_embed. \n",
    "Which is applied to same x for self-head attention\n",
    "\n",
    "and Qi = Wq * Ei = head_size*1 for each block_size and batch_size\n",
    "\n",
    "It can be represented as Linear(n_embed,head_size)\n",
    "\n",
    "How much each query vector attends to key vector is represented from dot product of Ki.Qi at each cell of matrix of size TxT\n",
    "\n",
    "this is represented by \n",
    "Attend = query @ key\n",
    "\n",
    "and the x is represented with the down projection to the dimension of head_size which is concatenated later\n",
    "Vi = Wv * Ei\n",
    "\n",
    "output from single head = attend @ Vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embed, head_size)\n",
    "        self.query = nn.Linear(n_embed, head_size)\n",
    "        self.value = nn.Linear(n_embed, head_size)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "    # x is a shape of Batch_size x Block_size x n_embed\n",
    "        key= self.key(x)        # Batch_size x Block_size x head_size ( B, T, C)\n",
    "        query = self.query(x)   # Batch_size x Block_size x head_size ( B, T, C)\n",
    "        # Batch_size x Block_size x head_size @ Batch_size x head_size * Block_size \n",
    "        attend = query @ key.transpose(-2, -1)  # Batch_size x Block_size x Block_size ( B, T, T)\n",
    "        attend = attend / (self.head_size ** 0.5)  #  Scaled Dot-Product Attention Attention(Q,K,V)=softmax(QK^T/sqrt(d_k))V  ( B, T, T)\n",
    "        trill = torch.tril(torch.ones(attend.shape[-1], attend.shape[-1]))  # Lower triangular matrix of block_size\n",
    "        attend = attend.masked_fill(trill == 0, float('-inf'))  # Masking future tokens\n",
    "        attend = torch.softmax(attend, dim=-1) # Column-wise softmax IG\n",
    "\n",
    "        value = self.value(x) # Batch_size x Block_size x head_size ( B, T, head_size)\n",
    "\n",
    "        out = attend @ value  # Batch_size x Block_size x head_size   ( B, T, head_size) \n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45b9ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = self.n_embed // self.n_heads\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(n_embed, self.head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)  # Concatenate outputs from all heads\n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)  # ( B, T, C)\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90f2de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    # Multi-layer perceptron (MLP) for feed-forward network in transformer\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "        nn.Linear(n_embed, 4 * n_embed),  # Up-projection min of 4* n_embed from the paper Attention Is All You Need\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4 * n_embed, n_embed),  # Down-projection back to n_embed\n",
    "        nn.Dropout(dropout)\n",
    "        )\n",
    "     \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "606d06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Bloack of the Transformer\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_embed, n_heads)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Creating a residual connection ( Skip Connection ) around the attention layer\n",
    "        x = x + self.attention(self.ln1(x))  # Layer normalization before attention :- Pre-Normalization\n",
    "        x = x + self.feed_forward(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04a04eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 120\n"
     ]
    }
   ],
   "source": [
    "## The final Transformer Block \n",
    "print(vocab_size, n_embed)\n",
    "class AakritiTransformer(nn.Module):\n",
    "    def __init__(self, n_layers, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.pos_embedding_table =  nn.Embedding(block_size,n_embed )\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_embed = n_embed\n",
    "        self.linear = nn.Linear(n_embed, vocab_size)\n",
    "        self.bn1 = nn.BatchNorm1d(vocab_size)\n",
    "        self.n_heads = n_heads\n",
    "        self.network = nn.Sequential(\n",
    "            *[Block(self.n_embed, self.n_heads  ) for _ in range(self.n_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.token_embedding_table(x) + self.pos_embedding_table(x)\n",
    "        out = self.network(embeddings)\n",
    "        out = self.linear(out)\n",
    "        out = self.bn1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6cc13d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    }
   ],
   "source": [
    "print(n_layers, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b54ffd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AakritiTransformer(n_layers, n_embed, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93771e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "AakritiTransformer                                      --\n",
       "├─Embedding: 1-1                                        960,000\n",
       "├─Embedding: 1-2                                        15,360\n",
       "├─Linear: 1-3                                           968,000\n",
       "├─BatchNorm1d: 1-4                                      16,000\n",
       "├─Sequential: 1-5                                       --\n",
       "│    └─Block: 2-1                                       --\n",
       "│    │    └─MultiHeadAttention: 3-1                     58,080\n",
       "│    │    └─FeedForward: 3-2                            115,800\n",
       "│    │    └─LayerNorm: 3-3                              240\n",
       "│    │    └─LayerNorm: 3-4                              240\n",
       "│    └─Block: 2-2                                       --\n",
       "│    │    └─MultiHeadAttention: 3-5                     58,080\n",
       "│    │    └─FeedForward: 3-6                            115,800\n",
       "│    │    └─LayerNorm: 3-7                              240\n",
       "│    │    └─LayerNorm: 3-8                              240\n",
       "│    └─Block: 2-3                                       --\n",
       "│    │    └─MultiHeadAttention: 3-9                     58,080\n",
       "│    │    └─FeedForward: 3-10                           115,800\n",
       "│    │    └─LayerNorm: 3-11                             240\n",
       "│    │    └─LayerNorm: 3-12                             240\n",
       "│    └─Block: 2-4                                       --\n",
       "│    │    └─MultiHeadAttention: 3-13                    58,080\n",
       "│    │    └─FeedForward: 3-14                           115,800\n",
       "│    │    └─LayerNorm: 3-15                             240\n",
       "│    │    └─LayerNorm: 3-16                             240\n",
       "│    └─Block: 2-5                                       --\n",
       "│    │    └─MultiHeadAttention: 3-17                    58,080\n",
       "│    │    └─FeedForward: 3-18                           115,800\n",
       "│    │    └─LayerNorm: 3-19                             240\n",
       "│    │    └─LayerNorm: 3-20                             240\n",
       "│    └─Block: 2-6                                       --\n",
       "│    │    └─MultiHeadAttention: 3-21                    58,080\n",
       "│    │    └─FeedForward: 3-22                           115,800\n",
       "│    │    └─LayerNorm: 3-23                             240\n",
       "│    │    └─LayerNorm: 3-24                             240\n",
       "│    └─Block: 2-7                                       --\n",
       "│    │    └─MultiHeadAttention: 3-25                    58,080\n",
       "│    │    └─FeedForward: 3-26                           115,800\n",
       "│    │    └─LayerNorm: 3-27                             240\n",
       "│    │    └─LayerNorm: 3-28                             240\n",
       "│    └─Block: 2-8                                       --\n",
       "│    │    └─MultiHeadAttention: 3-29                    58,080\n",
       "│    │    └─FeedForward: 3-30                           115,800\n",
       "│    │    └─LayerNorm: 3-31                             240\n",
       "│    │    └─LayerNorm: 3-32                             240\n",
       "================================================================================\n",
       "Total params: 3,354,240\n",
       "Trainable params: 3,354,240\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchinfo import summary\n",
    "from torchviz import make_dot\n",
    "\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d0eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
