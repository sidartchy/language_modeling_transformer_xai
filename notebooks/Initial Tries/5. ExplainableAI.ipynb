{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eec501b",
   "metadata": {},
   "source": [
    "# Perturbation-Based Feature Attribution\n",
    "\n",
    "1. The Basic Idea\n",
    "\n",
    "    \"If I remove/change this token, how much does the prediction change?\"\n",
    "\n",
    "2. Step-by-Step Process\n",
    "\n",
    "    a. Get Baseline: Run the model on the original sequence → get prediction + confidence \n",
    "\n",
    "    b. For each token position:\n",
    "      \n",
    "        -- Create a modified version (mask the token or replace with random)\n",
    "        -- Run the model on this modified sequence\n",
    "        -- Measure how much the confidence for the original prediction dropped\n",
    "        -- Repeat this multiple times for statistical stability\n",
    "\n",
    "\n",
    "3. Calculate Importance: Average the confidence drops across multiple perturbations\n",
    "\n",
    "\n",
    "\n",
    "## The Two Perturbation Strategies\n",
    "\n",
    "1. Masking: Removes information (what happens without this token?)\n",
    "2. Random: Adds noise (what happens with meaningless information here?)\n",
    "\n",
    "\n",
    "\n",
    "## Perturbation Method Mathematics\n",
    "Question: \"What happens to the prediction if I remove this specific token?\"\n",
    "\n",
    "Math: φᵢ = f(x) - E[f(x with token i masked/randomized)]\n",
    " \n",
    "Where:\n",
    "\n",
    "    f(x) = baseline prediction confidence\n",
    "    E[f(x⁽ⁱ⁾)] = expected prediction confidence when feature i is perturbed\n",
    "\n",
    "The expectation is over multiple random perturbations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9020c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple explanation method that doesn't require SHAP\n",
    "# This is much more memory-efficient and easier to use\n",
    "# Enhanced version with token decoding\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "def explain_transformer_simple(model, input_sequence, tokenizer, n_perturbations=50, method='mask'):\n",
    "    \"\"\"\n",
    "    Simple explanation method using perturbation analysis\n",
    "\n",
    "    Args:\n",
    "        model: Your trained transformer model\n",
    "        input_sequence: Input token sequence to explain\n",
    "        tokenizer: SentencePiece tokenizer for decoding tokens\n",
    "        n_perturbations: Number of perturbations per token\n",
    "        method: 'mask' (replace with 0) or 'random' (replace with random token)\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    # Convert to tensor if needed\n",
    "    if isinstance(input_sequence, np.ndarray):\n",
    "        input_sequence = input_sequence.tolist()\n",
    "\n",
    "    # Get baseline prediction\n",
    "    input_tensor = torch.tensor([input_sequence], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        baseline_logics, _, attention_weights = model(input_tensor)\n",
    "        baseline_probs = torch.softmax(baseline_logics[0, -1, :], dim=-1)\n",
    "        baseline_prediction = torch.argmax(baseline_probs).item()\n",
    "        baseline_confidence = baseline_probs[baseline_prediction].item()\n",
    "\n",
    "    # Decode the predicted token\n",
    "    predicted_token_text = tokenizer.decode([baseline_prediction])\n",
    "    print(f\"Baseline prediction: '{predicted_token_text}' (token {baseline_prediction}, confidence: {baseline_confidence:.4f})\")\n",
    "\n",
    "    importance_scores = np.zeros(len(input_sequence))\n",
    "\n",
    "    # Test importance of each position\n",
    "    for pos in range(len(input_sequence)):\n",
    "        impacts = []\n",
    "\n",
    "        for _ in range(n_perturbations):\n",
    "            # Create perturbed sequence\n",
    "            perturbed_seq = input_sequence.copy()\n",
    "\n",
    "            if method == 'mask':\n",
    "                perturbed_seq[pos] = 0  # Mask token\n",
    "            else:  # random\n",
    "                perturbed_seq[pos] = np.random.randint(1, 5000)\n",
    "\n",
    "            # Get perturbed prediction\n",
    "            perturbed_tensor = torch.tensor([perturbed_seq], dtype=torch.long).to(device)\n",
    "            with torch.no_grad():\n",
    "                perturbed_logics, _, _ = model(perturbed_tensor)\n",
    "                perturbed_probs = torch.softmax(perturbed_logics[0, -1, :], dim=-1)\n",
    "                perturbed_confidence = perturbed_probs[baseline_prediction].item()\n",
    "\n",
    "                # Calculate impact (drop in confidence for original prediction)\n",
    "                impact = baseline_confidence - perturbed_confidence\n",
    "                impacts.append(impact)\n",
    "\n",
    "        importance_scores[pos] = np.mean(impacts)\n",
    "\n",
    "        # Progress indicator\n",
    "        if (pos + 1) % 10 == 0 or pos == len(input_sequence) - 1:\n",
    "            print(f\"Progress: {pos + 1}/{len(input_sequence)} positions analyzed\")\n",
    "\n",
    "    return importance_scores, baseline_prediction, baseline_confidence, attention_weights, predicted_token_text\n",
    "\n",
    "def plot_explanation_results(input_sequence, importance_scores, attention_weights,\n",
    "                           predicted_token, predicted_token_text, confidence, tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Plot explanation results with multiple views and decoded tokens\n",
    "    \"\"\"\n",
    "    # Create figure with subplots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "    # 1. Token importance scores\n",
    "    positions = range(len(importance_scores))\n",
    "    colors = ['red' if score < 0 else 'blue' for score in importance_scores]\n",
    "\n",
    "    ax1.bar(positions, importance_scores, color=colors, alpha=0.7)\n",
    "    ax1.set_title(f'Token Importance Scores\\nPredicted: \"{predicted_token_text}\" (conf: {confidence:.3f})')\n",
    "    ax1.set_xlabel('Token Position')\n",
    "    ax1.set_ylabel('Importance Score')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Top influential tokens with decoded text\n",
    "    top_indices = np.argsort(np.abs(importance_scores))[-top_k:][::-1]\n",
    "    top_scores = importance_scores[top_indices]\n",
    "    top_colors = ['red' if score < 0 else 'blue' for score in top_scores]\n",
    "\n",
    "    # Decode tokens for labels\n",
    "    token_labels = []\n",
    "    for idx in top_indices:\n",
    "        token_id = input_sequence[idx]\n",
    "        try:\n",
    "            # Decode individual token\n",
    "            decoded_token = tokenizer.decode([token_id])\n",
    "            # Clean up the decoded token (remove special characters, limit length)\n",
    "            if len(decoded_token) > 10:\n",
    "                decoded_token = decoded_token[:7] + \"...\"\n",
    "            # Replace newlines and tabs with visible characters\n",
    "            decoded_token = decoded_token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "            token_labels.append(f\"pos_{idx}\\n'{decoded_token}'\\n(id_{token_id})\")\n",
    "        except:\n",
    "            # Fallback if decoding fails\n",
    "            token_labels.append(f\"pos_{idx}\\n(id_{token_id})\")\n",
    "\n",
    "    ax2.barh(range(len(top_indices)), top_scores, color=top_colors, alpha=0.7)\n",
    "    ax2.set_yticks(range(len(top_indices)))\n",
    "    ax2.set_yticklabels(token_labels, fontsize=8)\n",
    "    ax2.set_title(f'Top {top_k} Most Influential Tokens')\n",
    "    ax2.set_xlabel('Importance Score')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Attention weights (last layer, first head)\n",
    "    if attention_weights is not None and len(attention_weights) > 0:\n",
    "        # Get attention from last layer, first head, for the last token\n",
    "        last_layer_attention = attention_weights[-1][0, 0, -1, :].cpu().numpy()\n",
    "\n",
    "        ax3.bar(positions, last_layer_attention, alpha=0.7, color='green')\n",
    "        ax3.set_title('Attention Weights (Last Layer, Head 0)')\n",
    "        ax3.set_xlabel('Token Position')\n",
    "        ax3.set_ylabel('Attention Weight')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Importance vs Attention comparison\n",
    "        ax4.scatter(last_layer_attention, importance_scores, alpha=0.7)\n",
    "        ax4.set_xlabel('Attention Weight')\n",
    "        ax4.set_ylabel('Importance Score')\n",
    "        ax4.set_title('Importance vs Attention Correlation')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add correlation coefficient\n",
    "        correlation = np.corrcoef(last_layer_attention, importance_scores)[0, 1]\n",
    "        ax4.text(0.05, 0.95, f'Correlation: {correlation:.3f}',\n",
    "                transform=ax4.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Attention weights not available',\n",
    "                ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax4.text(0.5, 0.5, 'Attention weights not available',\n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_explanation(input_sequence, importance_scores, predicted_token, predicted_token_text, tokenizer, top_k=10):\n",
    "    \"\"\"\n",
    "    Provide detailed analysis of the explanation with decoded tokens\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPLANATION ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"Input sequence length: {len(input_sequence)}\")\n",
    "    print(f\"Predicted next token: '{predicted_token_text}' (ID: {predicted_token})\")\n",
    "    print(f\"Total importance score: {np.sum(importance_scores):.4f}\")\n",
    "\n",
    "    # Show a portion of the decoded input sequence\n",
    "    try:\n",
    "        decoded_input = tokenizer.decode(input_sequence[:50])  # First 50 tokens\n",
    "        print(f\"Input text (first ~50 tokens): '{decoded_input[:200]}{'...' if len(decoded_input) > 200 else ''}'\")\n",
    "    except:\n",
    "        print(\"Could not decode input sequence\")\n",
    "\n",
    "    # Positive and negative influences\n",
    "    positive_influence = np.sum(importance_scores[importance_scores > 0])\n",
    "    negative_influence = np.sum(importance_scores[importance_scores < 0])\n",
    "\n",
    "    print(f\"Positive influence: {positive_influence:.4f}\")\n",
    "    print(f\"Negative influence: {negative_influence:.4f}\")\n",
    "\n",
    "    # Top influential tokens\n",
    "    sorted_indices = np.argsort(np.abs(importance_scores))[::-1]\n",
    "\n",
    "    print(f\"\\nTop {top_k} most influential tokens:\")\n",
    "    print(f\"{'Rank':<4} {'Position':<8} {'Token Text':<15} {'Token ID':<8} {'Score':<10} {'Type'}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for i, idx in enumerate(sorted_indices[:top_k]):\n",
    "        try:\n",
    "            token_text = tokenizer.decode([input_sequence[idx]])\n",
    "            # Clean up token text for display\n",
    "            if len(token_text) > 12:\n",
    "                token_text = token_text[:9] + \"...\"\n",
    "            token_text = token_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "        except:\n",
    "            token_text = \"<?>\"\n",
    "\n",
    "        influence_type = \"Helpful\" if importance_scores[idx] > 0 else \"Harmful\"\n",
    "        print(f\"{i+1:<4} {idx:<8} '{token_text}'{'':.<{15-len(token_text)-2}} {input_sequence[idx]:<8} {importance_scores[idx]:<10.4f} {influence_type}\")\n",
    "\n",
    "    return {\n",
    "        'top_indices': sorted_indices[:top_k],\n",
    "        'positive_influence': positive_influence,\n",
    "        'negative_influence': negative_influence,\n",
    "        'total_influence': np.sum(importance_scores)\n",
    "    }\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Starting simple transformer explanation with token decoding...\")\n",
    "\n",
    "# # Get sample sequence\n",
    "# sample_idx = 100\n",
    "# sample_sequence = train_token_ids[sample_idx:sample_idx+block_size].tolist()\n",
    "\n",
    "\n",
    "sample_sequence = encoded.tolist()\n",
    "print(f\"Sample sequence length: {len(sample_sequence)}\")\n",
    "print(f\"First 10 tokens: {sample_sequence[:10]}\")\n",
    "\n",
    "# Show decoded version of first few tokens\n",
    "try:\n",
    "    decoded_sample = sp.decode(sample_sequence[:20])\n",
    "    print(f\"First ~20 tokens decoded: '{decoded_sample}'\")\n",
    "except:\n",
    "    print(\"Could not decode sample tokens\")\n",
    "\n",
    "# Run explanation (now requires tokenizer parameter)\n",
    "print(\"\\nComputing token importance scores...\")\n",
    "importance_scores, predicted_token, confidence, attention_weights, predicted_token_text = explain_transformer_simple(\n",
    "    model,\n",
    "    sample_sequence,\n",
    "    sp,  # Pass your SentencePiece tokenizer\n",
    "    n_perturbations=30,  # Adjust based on your patience/accuracy needs\n",
    "    method='mask'  # or 'random'\n",
    ")\n",
    "\n",
    "print(\"Explanation complete!\")\n",
    "\n",
    "# Plot results with decoded tokens\n",
    "plot_explanation_results(sample_sequence, importance_scores, attention_weights,\n",
    "                        predicted_token, predicted_token_text, confidence, sp, top_k=15)\n",
    "\n",
    "# Analyze results with decoded tokens\n",
    "analysis = analyze_explanation(sample_sequence, importance_scores, predicted_token,\n",
    "                             predicted_token_text, sp, top_k=10)\n",
    "\n",
    "# Clean up memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nAnalysis complete!\")\n",
    "print(\"This method shows which tokens in your input sequence most influence the model's next token prediction.\")\n",
    "print(\"Positive scores = helpful for the prediction, Negative scores = harmful for the prediction\")\n",
    "print(\"Now with human-readable token text for better interpretation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585fc5d",
   "metadata": {},
   "source": [
    "# SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP is a method that enables a fast computation of Shapley values and can be used to explain the prediction of an instance x by computing the contribution (Shapley value) of each feature to the prediction. SHAP values are based on the concept of Shapley values from cooperative game theory. \n",
    "\n",
    "\n",
    "## Mathematical Difference from Perturbation Method\n",
    "\n",
    "Question: \"What is the fair contribution of this token across all possible contexts?\"\n",
    "\n",
    "Math: φᵢ = Σ [weighted marginal contributions across all possible token subsets]\n",
    "\n",
    "<br>\n",
    "\n",
    "**Perturbation is actually a special case of SHAP! Here's why:**\n",
    "**SHAP considers all possible combinations of tokens:**\n",
    "\n",
    "    -- What if token i joins an empty sequence?\n",
    "    -- What if token i joins just token j?\n",
    "    -- What if token i joins tokens j and k?\n",
    "    -- What if token i joins ALL other tokens?\n",
    "\n",
    "**Perturbation only looks at the last case - token i joining all other tokens.**\n",
    "\n",
    "\n",
    "\n",
    "- SHAP systematically tries *every* coalition, measuring how token \\(i\\) helps in all possible “teams” of other tokens.\n",
    "- Perturbation methods only consider a **single coalition**:\n",
    "  - the “team” containing **all other tokens** except \\(i\\).\n",
    "\n",
    "For example, consider the token “quick” in the phrase *\"the quick brown fox\"*:\n",
    "\n",
    "- **SHAP** will test:\n",
    "  - Coalition {} (empty): (f({quick}) - f({}))\n",
    "  - Coalition {brown}: f({brown, quick}) - f({brown})\n",
    "  - Coalition {the, fox}: f({the, quick, fox}) - f({the, fox})\n",
    "  - Coalition {the, brown, fox}: f({the, brown, quick, fox}) - f({the, brown, fox})\n",
    "  - … and every other subset not including feature i (i.e quick)\n",
    "\n",
    "- **Perturbation** only looks at the “full context”:\n",
    "  - f({the, brown, fox, quick}) - f({the, brown, fox})\n",
    "\n",
    "Hence perturbation is equivalent to **one marginal contribution**, while SHAP aggregates **all** marginal contributions across all possible coalitions, providing a much fairer and more theoretically sound explanation.\n",
    "\n",
    "\n",
    "**In summary:**\n",
    "- **SHAP**: *“token’s contribution averaged over every possible subset of teammates”*\n",
    "- **Perturbation**: *“token’s contribution in just the full team”*\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad616b0a",
   "metadata": {},
   "source": [
    "# SHAP Approximate\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides a fair way to measure how each token contributes to the model’s prediction, inspired by cooperative game theory.\n",
    "\n",
    "\n",
    "## 1. Approximate Shapley Value Estimation\n",
    "\n",
    "We sample random subsets of tokens (coalitions).\n",
    "\n",
    "- Let **x** be the token sequence of length *n*  \n",
    "- Let **z** be a binary mask:\n",
    "  - `z_i = 1` → token present\n",
    "  - `z_i = 0` → token perturbed\n",
    "\n",
    "Each coalition defines a perturbed sequence:\n",
    "\n",
    "$$\n",
    "\\tilde{x}^{(j)} = x \\odot z^{(j)} + p \\odot (1 - z^{(j)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- *p* is the perturbation baseline (e.g. masked token)\n",
    "- $\\odot$ is elementwise product\n",
    "\n",
    " \n",
    "\n",
    "## 2. Model Output Collection\n",
    "\n",
    "For each coalition:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{(j)} = f(\\tilde{x}^{(j)})\n",
    "$$\n",
    "\n",
    "Measure change in confidence:\n",
    "\n",
    "$$\n",
    "c^{(j)} = \\hat{y}^{(j)} - \\hat{y}^{\\text{baseline}}\n",
    "$$\n",
    "\n",
    "where $\\hat{y}^{\\text{baseline}} = f(x)$ is the original confidence.\n",
    "\n",
    " \n",
    "\n",
    "## 3. Linear Shapley Approximation\n",
    "\n",
    "Collect *m* such samples, and fit a simple regression:\n",
    "\n",
    "$$\n",
    "c^{(j)} \\approx z^{(j)} \\cdot \\phi\n",
    "$$\n",
    "\n",
    "so that\n",
    "\n",
    "$$\n",
    "\\phi \\in \\mathbb{R}^n\n",
    "$$\n",
    "\n",
    "is the approximate Shapley value vector.\n",
    "\n",
    "\n",
    "\n",
    "## 4. Token Perturbation\n",
    "\n",
    "If a token is removed $(z_i=0)$, it is replaced with:\n",
    "- token id 0 (mask)\n",
    "- or a random token\n",
    "\n",
    "\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "    Sample random coalitions  \n",
    "    Perturb tokens  \n",
    "    Measure change in prediction  \n",
    "    Fit regression to get approximate Shapley values\n",
    "\n",
    "\n",
    "## 6. Final Takeaway\n",
    "\n",
    "**Linear SHAP** is a fast approximation:  \n",
    "it fits a linear model over random coalitions  \n",
    "to estimate how much each token helps the final confidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple explanation method that doesn't require SHAP\n",
    "# This is much more memory-efficient and easier to use\n",
    "# Enhanced version with token decoding\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "def shap_approx_explanation(model, input_sequence, tokenizer, n_samples=100, method='mask'):\n",
    "    \"\"\"\n",
    "    SHAP-style approximate explanation by sampling coalitions (subsets of tokens).\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model.\n",
    "        input_sequence: List of token ids.\n",
    "        tokenizer: The tokenizer for decoding tokens.\n",
    "        n_samples: Number of subset samples (coalitions) to generate.\n",
    "        method: How to perturb tokens ('mask' or 'random').\n",
    "\n",
    "    Returns:\n",
    "        shap_values: Array of Shapley value estimates for each token position.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    input_sequence = np.array(input_sequence)\n",
    "    num_tokens = len(input_sequence)\n",
    "    \n",
    "    # Baseline output (all tokens as-is)\n",
    "    input_tensor = torch.tensor([input_sequence], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        base_logits, _, _ = model(input_tensor)\n",
    "        base_probs = torch.softmax(base_logits[0, -1, :], dim=-1)\n",
    "        base_pred = torch.argmax(base_probs).item()\n",
    "        base_conf = base_probs[base_pred].item()\n",
    "    \n",
    "    print(f\"Baseline prediction: '{tokenizer.decode([base_pred])}' (token {base_pred}, confidence: {base_conf:.4f})\")\n",
    "\n",
    "    # Prepare storage for coalition data\n",
    "    coalition_matrix = []\n",
    "    outputs = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Random mask pattern: 1 = keep, 0 = perturb\n",
    "        coalition = np.random.randint(0, 2, num_tokens)\n",
    "        perturbed_seq = input_sequence.copy()\n",
    "\n",
    "        for pos in range(num_tokens):\n",
    "            if coalition[pos] == 0:\n",
    "                if method == 'mask':\n",
    "                    perturbed_seq[pos] = 0\n",
    "                else:\n",
    "                    perturbed_seq[pos] = np.random.randint(1, 5000)\n",
    "        \n",
    "        perturbed_tensor = torch.tensor([perturbed_seq], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, _, _ = model(perturbed_tensor)\n",
    "            probs = torch.softmax(logits[0, -1, :], dim=-1)\n",
    "            conf = probs[base_pred].item()\n",
    "        \n",
    "        coalition_matrix.append(coalition)\n",
    "        outputs.append(conf)\n",
    "    \n",
    "    coalition_matrix = np.array(coalition_matrix)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    # Solve weighted linear regression to get SHAP values\n",
    "    # (no kernel weighting here for simplicity, but could be added)\n",
    "    X = coalition_matrix\n",
    "    y = outputs - base_conf  # deviation from baseline\n",
    "    shap_values, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "\n",
    "    return shap_values, base_pred, base_conf\n",
    "\n",
    "# Example usage:\n",
    "# shap_values, predicted_token, confidence = shap_approx_explanation(model, sample_sequence, sp, n_samples=200, method='mask')\n",
    "\n",
    "def plot_explanation_results(input_sequence, importance_scores,\n",
    "                             predicted_token, predicted_token_text, confidence,\n",
    "                             tokenizer, top_k=15):\n",
    "    \"\"\"\n",
    "    Plot SHAP-style explanation results with decoded tokens.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # 1. Token importance scores\n",
    "    positions = range(len(importance_scores))\n",
    "    colors = ['red' if score < 0 else 'blue' for score in importance_scores]\n",
    "\n",
    "    ax1.bar(positions, importance_scores, color=colors, alpha=0.7)\n",
    "    ax1.set_title(f'Token Importance Scores\\nPredicted: \"{predicted_token_text}\" (conf: {confidence:.3f})')\n",
    "    ax1.set_xlabel('Token Position')\n",
    "    ax1.set_ylabel('Importance Score')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Top influential tokens with decoded text\n",
    "    top_indices = np.argsort(np.abs(importance_scores))[-top_k:][::-1]\n",
    "    top_scores = importance_scores[top_indices]\n",
    "    top_colors = ['red' if score < 0 else 'blue' for score in top_scores]\n",
    "\n",
    "    token_labels = []\n",
    "    for idx in top_indices:\n",
    "        token_id = input_sequence[idx]\n",
    "        try:\n",
    "            decoded_token = tokenizer.decode([token_id])\n",
    "            if len(decoded_token) > 10:\n",
    "                decoded_token = decoded_token[:7] + \"...\"\n",
    "            decoded_token = decoded_token.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "            token_labels.append(f\"pos_{idx}\\n'{decoded_token}'\\n(id_{token_id})\")\n",
    "        except:\n",
    "            token_labels.append(f\"pos_{idx}\\n(id_{token_id})\")\n",
    "\n",
    "    ax2.barh(range(len(top_indices)), top_scores, color=top_colors, alpha=0.7)\n",
    "    ax2.set_yticks(range(len(top_indices)))\n",
    "    ax2.set_yticklabels(token_labels, fontsize=8)\n",
    "    ax2.set_title(f'Top {top_k} Most Influential Tokens')\n",
    "    ax2.set_xlabel('Importance Score')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_explanation(input_sequence, importance_scores, predicted_token, predicted_token_text, tokenizer, top_k=10):\n",
    "    \"\"\"\n",
    "    Provide detailed analysis of the explanation with decoded tokens\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXPLANATION ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"Input sequence length: {len(input_sequence)}\")\n",
    "    print(f\"Predicted next token: '{predicted_token_text}' (ID: {predicted_token})\")\n",
    "    print(f\"Total importance score: {np.sum(importance_scores):.4f}\")\n",
    "\n",
    "    try:\n",
    "        decoded_input = tokenizer.decode(input_sequence[:50])\n",
    "        print(f\"Input text (first ~50 tokens): '{decoded_input[:200]}{'...' if len(decoded_input) > 200 else ''}'\")\n",
    "    except:\n",
    "        print(\"Could not decode input sequence\")\n",
    "\n",
    "    positive_influence = np.sum(importance_scores[importance_scores > 0])\n",
    "    negative_influence = np.sum(importance_scores[importance_scores < 0])\n",
    "\n",
    "    print(f\"Positive influence: {positive_influence:.4f}\")\n",
    "    print(f\"Negative influence: {negative_influence:.4f}\")\n",
    "\n",
    "    sorted_indices = np.argsort(np.abs(importance_scores))[::-1]\n",
    "\n",
    "    print(f\"\\nTop {top_k} most influential tokens:\")\n",
    "    print(f\"{'Rank':<4} {'Position':<8} {'Token Text':<15} {'Token ID':<8} {'Score':<10} {'Type'}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for i, idx in enumerate(sorted_indices[:top_k]):\n",
    "        try:\n",
    "            token_text = tokenizer.decode([input_sequence[idx]])\n",
    "            if len(token_text) > 12:\n",
    "                token_text = token_text[:9] + \"...\"\n",
    "            token_text = token_text.replace('\\n', '\\\\n').replace('\\t', '\\\\t')\n",
    "        except:\n",
    "            token_text = \"<?>\"\n",
    "\n",
    "        influence_type = \"Helpful\" if importance_scores[idx] > 0 else \"Harmful\"\n",
    "        print(f\"{i+1:<4} {idx:<8} '{token_text}'{'':.<{15-len(token_text)-2}} {input_sequence[idx]:<8} {importance_scores[idx]:<10.4f} {influence_type}\")\n",
    "\n",
    "    return {\n",
    "        'top_indices': sorted_indices[:top_k],\n",
    "        'positive_influence': positive_influence,\n",
    "        'negative_influence': negative_influence,\n",
    "        'total_influence': np.sum(importance_scores)\n",
    "    }\n",
    "\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Starting simple transformer explanation with token decoding...\")\n",
    "\n",
    "# # Get sample sequence\n",
    "# sample_idx = 100\n",
    "# sample_sequence = train_token_ids[sample_idx:sample_idx+block_size].tolist()\n",
    "\n",
    "\n",
    "sample_sequence = encoded.tolist()\n",
    "print(f\"Sample sequence length: {len(sample_sequence)}\")\n",
    "print(f\"First 10 tokens: {sample_sequence[:10]}\")\n",
    "\n",
    "# Show decoded version of first few tokens\n",
    "try:\n",
    "    decoded_sample = sp.decode(sample_sequence[:20])\n",
    "    print(f\"First ~20 tokens decoded: '{decoded_sample}'\")\n",
    "except:\n",
    "    print(\"Could not decode sample tokens\")\n",
    "\n",
    "# Run explanation (now requires tokenizer parameter)\n",
    "print(\"\\nComputing token importance scores...\")\n",
    "\n",
    "# Compute SHAP-style explanation\n",
    "importance_scores, predicted_token, confidence = shap_approx_explanation(\n",
    "    model,\n",
    "    sample_sequence,\n",
    "    tokenizer=sp,\n",
    "    n_samples=100,\n",
    "    method='mask'\n",
    ")\n",
    "predicted_token_text = sp.decode([predicted_token])\n",
    "\n",
    "# Plot\n",
    "plot_explanation_results(\n",
    "    sample_sequence,\n",
    "    importance_scores,\n",
    "    predicted_token,\n",
    "    predicted_token_text,\n",
    "    confidence,\n",
    "    tokenizer=sp,\n",
    "    top_k=15\n",
    ")\n",
    "\n",
    "# Analyze\n",
    "analyze_explanation(\n",
    "    sample_sequence,\n",
    "    importance_scores,\n",
    "    predicted_token,\n",
    "    predicted_token_text,\n",
    "    tokenizer=sp,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "# Clean up memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nAnalysis complete!\")\n",
    "print(\"This method shows which tokens in your input sequence most influence the model's next token prediction.\")\n",
    "print(\"Positive scores = helpful for the prediction, Negative scores = harmful for the prediction\")\n",
    "print(\"Now with human-readable token text for better interpretation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
