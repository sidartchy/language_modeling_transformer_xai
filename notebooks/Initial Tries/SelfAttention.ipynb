{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10302971",
   "metadata": {},
   "source": [
    "### Single Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762eec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106a338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Defining parameters for the transformer model\n",
    "\n",
    "n_embed = 120\n",
    "n_layers = 8\n",
    "n_heads = 8\n",
    "head_size = n_embed // n_heads\n",
    "block_size = 256  # Context size for the model\n",
    "dropout = 0.2  # Dropout rate for regularization\n",
    "vocab_size = 65_536   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbc8b5",
   "metadata": {},
   "source": [
    "We know that for each token it consist a embedding vector of dimension n_embed.\n",
    "\n",
    "i.e.. Ei vector of size n_embed*1                ,where i runs to block_size\n",
    "\n",
    "And for each head there is a query matrix and key matrix of size head_size*n_embed. \n",
    "Which is applied to same x for self-head attention\n",
    "\n",
    "and Qi = Wq * Ei = head_size*1 for each block_size and batch_size\n",
    "\n",
    "It can be represented as Linear(n_embed,head_size)\n",
    "\n",
    "How much each query vector attends to key vector is represented from dot product of Ki.Qi at each cell of matrix of size TxT\n",
    "\n",
    "this is represented by \n",
    "Attend = query @ key\n",
    "\n",
    "and the x is represented with the down projection to the dimension of head_size which is concatenated later\n",
    "Vi = Wv * Ei\n",
    "\n",
    "output from single head = attend @ Vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dfccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.module):\n",
    "\n",
    "    def _init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embed = n_embed\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embed, head_size)\n",
    "        self.query = nn.Linear(n_embed, head_size)\n",
    "        self.value = nn.Linear(n_embed, head_size)\n",
    " \n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            # x is a shape of Batch_size x Block_size x n_embed\n",
    "            key= self.key(x)        # Batch_size x Block_size x head_size\n",
    "            query = self.query(x)   # Batch_size x Block_size x head_size\n",
    "\n",
    "            # Batch_size x Block_size x head_size @ Batch_size x head_size * Block_size \n",
    "            attend = query @ key.transpose(-2, -1)  # Batch_size x Block_size x Block_size\n",
    "\n",
    "            attend = attend / (self.head_size ** 0.5)  #  Scaled Dot-Product Attention Attention(Q,K,V)=softmax(QK^T/sqrt(d_k))V\n",
    "\n",
    "            trill = torch.tril(torch.ones(attend.shape[-1], attend.shape[-1]))  # Lower triangular matrix of block_size\n",
    "\n",
    "            attend = attend.masked_fill(trill == 0, float('-inf'))  # Masking future tokens\n",
    "            attend = torch.softmax(attend, dim=-1)\n",
    "\n",
    "            value = self.value(x) # Batch_size x Block_size x head_size\n",
    "\n",
    "            out = attend @ value  # Batch_size x Block_size x head_size\n",
    "\n",
    "            return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f2de40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grahicsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
